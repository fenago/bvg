{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0S2D2PV64Dt"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain llama_index pypdf PyCryptodome gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9hxR1nk8L8_"
      },
      "outputs": [],
      "source": [
        "from llama_index import StorageContext, ServiceContext, GPTVectorStoreIndex, LLMPredictor, PromptHelper, SimpleDirectoryReader, load_index_from_storage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import gradio as gr\n",
        "import sys\n",
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdnCS1Tt8Uo5"
      },
      "outputs": [],
      "source": [
        "# Set your API key as an environment variable.\n",
        "os.environ['OPENAI_API_KEY'] = \"<ADD-KEY-HERE>\"\n",
        "openai.organization = \"org-AUCNtAn1zVfDly8kavDXWkyF\"\n",
        "\n",
        "# Use your API key.\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsYgDCQc8fn-"
      },
      "outputs": [],
      "source": [
        "def create_service_context():\n",
        "\n",
        "    #constraint parameters\n",
        "    max_input_size = 4096\n",
        "    num_outputs = 512\n",
        "    max_chunk_overlap = .5\n",
        "    chunk_size_limit = 600\n",
        "\n",
        "    #allows the user to explicitly set certain constraint parameters\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    #LLMPredictor is a wrapper class around LangChain's LLMChain that allows easy integration into LlamaIndex\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
        "\n",
        "    #constructs service_context\n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "    return service_context\n",
        "\n",
        "def data_ingestion_indexing(directory_path):\n",
        "\n",
        "    #loads data from the specified directory path\n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "\n",
        "    #when first building the index\n",
        "    index = GPTVectorStoreIndex.from_documents(\n",
        "        documents, service_context=create_service_context()\n",
        "    )\n",
        "\n",
        "    #persist index to disk, default \"storage\" folder\n",
        "    index.storage_context.persist()\n",
        "\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKA2WHef9LCP"
      },
      "outputs": [],
      "source": [
        "def data_querying(input_text):\n",
        "\n",
        "    #rebuild storage context\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "\n",
        "    #loads index from storage\n",
        "    index = load_index_from_storage(storage_context, service_context=create_service_context())\n",
        "\n",
        "    #queries the index with the input text\n",
        "    response = index.as_query_engine().query(input_text)\n",
        "\n",
        "    return response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9HFOWrZ9Q_f"
      },
      "outputs": [],
      "source": [
        "iface = gr.Interface(fn=data_querying,\n",
        "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
        "                     outputs=\"text\",\n",
        "                     title=\"Dr. Lee and the Custom-trained Machine Learning Knowledge Base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wixLxSnk9Y0w"
      },
      "outputs": [],
      "source": [
        "!pip install docx2txt\n",
        "#passes in data directory\n",
        "index = data_ingestion_indexing(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sevfbsisAoWg"
      },
      "outputs": [],
      "source": [
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CVY_j8Gl4Aw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkut5y-Yl39H"
      },
      "outputs": [],
      "source": [
        "!pip freeze >> requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYZTgS7krkW8"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkSKoktftfsF"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFhQ-3Jjtbc-"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores import ChromaVectorStore\n",
        "from llama_index import StorageContext\n",
        "\n",
        "chroma_client = chromadb.PersistentClient()\n",
        "chroma_collection = chroma_client.create_collection(\"jnj\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDUh2C0TxO-i"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is this document about?\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}